{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "Time      284807 non-null float64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "card.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_dup = card.copy()\n",
    "card_dup.drop_duplicates(subset=None,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283726, 31)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del card_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.Class.value_counts() #492 fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax=sns.countplot(x='Class',data=card);\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
    "\n",
    "X1 = card[estimators]\n",
    "y = card['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col=X1.columns[:-1]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003914\n",
      "         Iterations 13\n"
     ]
    }
   ],
   "source": [
    "X = sm.add_constant(X1)\n",
    "reg_logit = sm.Logit(y,X)\n",
    "results_logit = reg_logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>284807</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>284776</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    30</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 21 May 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.6922</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:05:40</td>     <th>  Log-Likelihood:    </th> <td> -1114.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -3621.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -8.3917</td> <td>    0.249</td> <td>  -33.652</td> <td> 0.000</td> <td>   -8.880</td> <td>   -7.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th>   <td>-3.742e-06</td> <td> 2.26e-06</td> <td>   -1.659</td> <td> 0.097</td> <td>-8.16e-06</td> <td> 6.79e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>     <td>    0.0960</td> <td>    0.042</td> <td>    2.264</td> <td> 0.024</td> <td>    0.013</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>     <td>    0.0094</td> <td>    0.058</td> <td>    0.161</td> <td> 0.872</td> <td>   -0.104</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>     <td>   -0.0079</td> <td>    0.053</td> <td>   -0.149</td> <td> 0.881</td> <td>   -0.112</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>     <td>    0.6986</td> <td>    0.074</td> <td>    9.454</td> <td> 0.000</td> <td>    0.554</td> <td>    0.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>     <td>    0.1295</td> <td>    0.067</td> <td>    1.944</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>     <td>   -0.1198</td> <td>    0.074</td> <td>   -1.626</td> <td> 0.104</td> <td>   -0.264</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>     <td>   -0.0969</td> <td>    0.067</td> <td>   -1.453</td> <td> 0.146</td> <td>   -0.228</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>     <td>   -0.1739</td> <td>    0.030</td> <td>   -5.711</td> <td> 0.000</td> <td>   -0.234</td> <td>   -0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>     <td>   -0.2843</td> <td>    0.111</td> <td>   -2.561</td> <td> 0.010</td> <td>   -0.502</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>    <td>   -0.8176</td> <td>    0.097</td> <td>   -8.432</td> <td> 0.000</td> <td>   -1.008</td> <td>   -0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>    <td>   -0.0621</td> <td>    0.081</td> <td>   -0.762</td> <td> 0.446</td> <td>   -0.222</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>    <td>    0.0909</td> <td>    0.087</td> <td>    1.045</td> <td> 0.296</td> <td>   -0.080</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>    <td>   -0.3312</td> <td>    0.082</td> <td>   -4.058</td> <td> 0.000</td> <td>   -0.491</td> <td>   -0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>    <td>   -0.5571</td> <td>    0.062</td> <td>   -8.949</td> <td> 0.000</td> <td>   -0.679</td> <td>   -0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>    <td>   -0.1141</td> <td>    0.086</td> <td>   -1.330</td> <td> 0.183</td> <td>   -0.282</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>    <td>   -0.1908</td> <td>    0.125</td> <td>   -1.526</td> <td> 0.127</td> <td>   -0.436</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>    <td>   -0.0216</td> <td>    0.070</td> <td>   -0.309</td> <td> 0.757</td> <td>   -0.159</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>    <td>   -0.0131</td> <td>    0.129</td> <td>   -0.102</td> <td> 0.919</td> <td>   -0.266</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>    <td>    0.0963</td> <td>    0.097</td> <td>    0.993</td> <td> 0.321</td> <td>   -0.094</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>    <td>   -0.4582</td> <td>    0.082</td> <td>   -5.607</td> <td> 0.000</td> <td>   -0.618</td> <td>   -0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>    <td>    0.3898</td> <td>    0.060</td> <td>    6.494</td> <td> 0.000</td> <td>    0.272</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>    <td>    0.6297</td> <td>    0.134</td> <td>    4.707</td> <td> 0.000</td> <td>    0.367</td> <td>    0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>    <td>   -0.0951</td> <td>    0.058</td> <td>   -1.629</td> <td> 0.103</td> <td>   -0.209</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>    <td>    0.1289</td> <td>    0.147</td> <td>    0.874</td> <td> 0.382</td> <td>   -0.160</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>    <td>   -0.0761</td> <td>    0.131</td> <td>   -0.582</td> <td> 0.560</td> <td>   -0.332</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>    <td>    0.0195</td> <td>    0.190</td> <td>    0.103</td> <td> 0.918</td> <td>   -0.352</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>    <td>   -0.8188</td> <td>    0.122</td> <td>   -6.686</td> <td> 0.000</td> <td>   -1.059</td> <td>   -0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>    <td>   -0.2937</td> <td>    0.088</td> <td>   -3.332</td> <td> 0.001</td> <td>   -0.467</td> <td>   -0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th> <td>    0.0009</td> <td>    0.000</td> <td>    2.449</td> <td> 0.014</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.31 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               284807\n",
       "Model:                          Logit   Df Residuals:                   284776\n",
       "Method:                           MLE   Df Model:                           30\n",
       "Date:                Fri, 21 May 2021   Pseudo R-squ.:                  0.6922\n",
       "Time:                        21:05:40   Log-Likelihood:                -1114.8\n",
       "converged:                       True   LL-Null:                       -3621.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -8.3917      0.249    -33.652      0.000      -8.880      -7.903\n",
       "Time       -3.742e-06   2.26e-06     -1.659      0.097   -8.16e-06    6.79e-07\n",
       "V1             0.0960      0.042      2.264      0.024       0.013       0.179\n",
       "V2             0.0094      0.058      0.161      0.872      -0.104       0.123\n",
       "V3            -0.0079      0.053     -0.149      0.881      -0.112       0.096\n",
       "V4             0.6986      0.074      9.454      0.000       0.554       0.843\n",
       "V5             0.1295      0.067      1.944      0.052      -0.001       0.260\n",
       "V6            -0.1198      0.074     -1.626      0.104      -0.264       0.025\n",
       "V7            -0.0969      0.067     -1.453      0.146      -0.228       0.034\n",
       "V8            -0.1739      0.030     -5.711      0.000      -0.234      -0.114\n",
       "V9            -0.2843      0.111     -2.561      0.010      -0.502      -0.067\n",
       "V10           -0.8176      0.097     -8.432      0.000      -1.008      -0.628\n",
       "V11           -0.0621      0.081     -0.762      0.446      -0.222       0.098\n",
       "V12            0.0909      0.087      1.045      0.296      -0.080       0.261\n",
       "V13           -0.3312      0.082     -4.058      0.000      -0.491      -0.171\n",
       "V14           -0.5571      0.062     -8.949      0.000      -0.679      -0.435\n",
       "V15           -0.1141      0.086     -1.330      0.183      -0.282       0.054\n",
       "V16           -0.1908      0.125     -1.526      0.127      -0.436       0.054\n",
       "V17           -0.0216      0.070     -0.309      0.757      -0.159       0.116\n",
       "V18           -0.0131      0.129     -0.102      0.919      -0.266       0.240\n",
       "V19            0.0963      0.097      0.993      0.321      -0.094       0.286\n",
       "V20           -0.4582      0.082     -5.607      0.000      -0.618      -0.298\n",
       "V21            0.3898      0.060      6.494      0.000       0.272       0.507\n",
       "V22            0.6297      0.134      4.707      0.000       0.367       0.892\n",
       "V23           -0.0951      0.058     -1.629      0.103      -0.209       0.019\n",
       "V24            0.1289      0.147      0.874      0.382      -0.160       0.418\n",
       "V25           -0.0761      0.131     -0.582      0.560      -0.332       0.180\n",
       "V26            0.0195      0.190      0.103      0.918      -0.352       0.392\n",
       "V27           -0.8188      0.122     -6.686      0.000      -1.059      -0.579\n",
       "V28           -0.2937      0.088     -3.332      0.001      -0.467      -0.121\n",
       "Amount         0.0009      0.000      2.449      0.014       0.000       0.002\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.31 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_logit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def back_feature_elem (data_frame,dep_var,col_list):\n",
    "    \"\"\" Takes in the dataframe, the dependent variable and a list of column names, runs the regression repeatedly eleminating feature with the highest\n",
    "    P-value above alpha one at a time and returns the regression summary with all p-values below alpha\"\"\"\n",
    "\n",
    "    while len(col_list)>0 :\n",
    "        model=sm.Logit(dep_var,data_frame[col_list])\n",
    "        result=model.fit(disp=0)\n",
    "        largest_pvalue=round(result.pvalues,3).nlargest(1)\n",
    "        if largest_pvalue[0]<(0.0001):\n",
    "            return result\n",
    "            break\n",
    "        else:\n",
    "            col_list=col_list.drop(largest_pvalue.index)\n",
    "\n",
    "result=back_feature_elem(X,card.Class,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>284807</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>284782</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    24</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 21 May 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.06233</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:05:53</td>     <th>  Log-Likelihood:    </th> <td> -3395.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -3621.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.919e-80</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th> <td>   -0.0001</td> <td> 1.43e-06</td> <td>  -86.049</td> <td> 0.000</td> <td>   -0.000</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>   <td>    0.8905</td> <td>    0.028</td> <td>   31.948</td> <td> 0.000</td> <td>    0.836</td> <td>    0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>   <td>   -0.4531</td> <td>    0.023</td> <td>  -19.454</td> <td> 0.000</td> <td>   -0.499</td> <td>   -0.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>   <td>   -1.6040</td> <td>    0.032</td> <td>  -49.454</td> <td> 0.000</td> <td>   -1.668</td> <td>   -1.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>   <td>    0.1445</td> <td>    0.025</td> <td>    5.668</td> <td> 0.000</td> <td>    0.095</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>   <td>    0.4081</td> <td>    0.024</td> <td>   17.097</td> <td> 0.000</td> <td>    0.361</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>   <td>   -0.3890</td> <td>    0.025</td> <td>  -15.311</td> <td> 0.000</td> <td>   -0.439</td> <td>   -0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>   <td>    0.0992</td> <td>    0.028</td> <td>    3.582</td> <td> 0.000</td> <td>    0.045</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>   <td>   -0.3893</td> <td>    0.023</td> <td>  -17.118</td> <td> 0.000</td> <td>   -0.434</td> <td>   -0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>   <td>   -0.4601</td> <td>    0.043</td> <td>  -10.675</td> <td> 0.000</td> <td>   -0.545</td> <td>   -0.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>  <td>   -0.3799</td> <td>    0.051</td> <td>   -7.436</td> <td> 0.000</td> <td>   -0.480</td> <td>   -0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>  <td>   -0.6133</td> <td>    0.034</td> <td>  -17.969</td> <td> 0.000</td> <td>   -0.680</td> <td>   -0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>  <td>    0.1263</td> <td>    0.034</td> <td>    3.678</td> <td> 0.000</td> <td>    0.059</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>  <td>   -0.4513</td> <td>    0.035</td> <td>  -13.070</td> <td> 0.000</td> <td>   -0.519</td> <td>   -0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>  <td>   -0.6980</td> <td>    0.032</td> <td>  -22.083</td> <td> 0.000</td> <td>   -0.760</td> <td>   -0.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>  <td>   -1.0426</td> <td>    0.041</td> <td>  -25.255</td> <td> 0.000</td> <td>   -1.123</td> <td>   -0.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>  <td>   -0.2386</td> <td>    0.042</td> <td>   -5.747</td> <td> 0.000</td> <td>   -0.320</td> <td>   -0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>  <td>   -0.7041</td> <td>    0.033</td> <td>  -21.507</td> <td> 0.000</td> <td>   -0.768</td> <td>   -0.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>  <td>   -0.6758</td> <td>    0.050</td> <td>  -13.576</td> <td> 0.000</td> <td>   -0.773</td> <td>   -0.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>  <td>    0.5683</td> <td>    0.041</td> <td>   13.871</td> <td> 0.000</td> <td>    0.488</td> <td>    0.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>  <td>    1.3555</td> <td>    0.062</td> <td>   21.869</td> <td> 0.000</td> <td>    1.234</td> <td>    1.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>  <td>    0.2932</td> <td>    0.054</td> <td>    5.408</td> <td> 0.000</td> <td>    0.187</td> <td>    0.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>  <td>   -1.9974</td> <td>    0.072</td> <td>  -27.593</td> <td> 0.000</td> <td>   -2.139</td> <td>   -1.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>  <td>    0.3076</td> <td>    0.072</td> <td>    4.281</td> <td> 0.000</td> <td>    0.167</td> <td>    0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>  <td>   -0.7474</td> <td>    0.088</td> <td>   -8.483</td> <td> 0.000</td> <td>   -0.920</td> <td>   -0.575</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.68 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               284807\n",
       "Model:                          Logit   Df Residuals:                   284782\n",
       "Method:                           MLE   Df Model:                           24\n",
       "Date:                Fri, 21 May 2021   Pseudo R-squ.:                 0.06233\n",
       "Time:                        21:05:53   Log-Likelihood:                -3395.5\n",
       "converged:                       True   LL-Null:                       -3621.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.919e-80\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Time          -0.0001   1.43e-06    -86.049      0.000      -0.000      -0.000\n",
       "V1             0.8905      0.028     31.948      0.000       0.836       0.945\n",
       "V2            -0.4531      0.023    -19.454      0.000      -0.499      -0.407\n",
       "V3            -1.6040      0.032    -49.454      0.000      -1.668      -1.540\n",
       "V4             0.1445      0.025      5.668      0.000       0.095       0.195\n",
       "V5             0.4081      0.024     17.097      0.000       0.361       0.455\n",
       "V6            -0.3890      0.025    -15.311      0.000      -0.439      -0.339\n",
       "V7             0.0992      0.028      3.582      0.000       0.045       0.154\n",
       "V8            -0.3893      0.023    -17.118      0.000      -0.434      -0.345\n",
       "V9            -0.4601      0.043    -10.675      0.000      -0.545      -0.376\n",
       "V10           -0.3799      0.051     -7.436      0.000      -0.480      -0.280\n",
       "V11           -0.6133      0.034    -17.969      0.000      -0.680      -0.546\n",
       "V12            0.1263      0.034      3.678      0.000       0.059       0.194\n",
       "V13           -0.4513      0.035    -13.070      0.000      -0.519      -0.384\n",
       "V14           -0.6980      0.032    -22.083      0.000      -0.760      -0.636\n",
       "V15           -1.0426      0.041    -25.255      0.000      -1.123      -0.962\n",
       "V16           -0.2386      0.042     -5.747      0.000      -0.320      -0.157\n",
       "V17           -0.7041      0.033    -21.507      0.000      -0.768      -0.640\n",
       "V20           -0.6758      0.050    -13.576      0.000      -0.773      -0.578\n",
       "V21            0.5683      0.041     13.871      0.000       0.488       0.649\n",
       "V22            1.3555      0.062     21.869      0.000       1.234       1.477\n",
       "V23            0.2932      0.054      5.408      0.000       0.187       0.399\n",
       "V25           -1.9974      0.072    -27.593      0.000      -2.139      -1.855\n",
       "V26            0.3076      0.072      4.281      0.000       0.167       0.448\n",
       "V27           -0.7474      0.088     -8.483      0.000      -0.920      -0.575\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.68 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CI 95%(2.5%)  CI 95%(97.5%)  Odds Ratio  pvalue\n",
      "Time      0.999874       0.999880    0.999877     0.0\n",
      "V1        2.306871       2.573219    2.436408     0.0\n",
      "V2        0.607258       0.665317    0.635624     0.0\n",
      "V3        0.188703       0.214287    0.201088     0.0\n",
      "V4        1.099162       1.214710    1.155493     0.0\n",
      "V5        1.435225       1.575997    1.503965     0.0\n",
      "V6        0.644844       0.712365    0.677764     0.0\n",
      "V7        1.045965       1.165922    1.104316     0.0\n",
      "V8        0.647980       0.708401    0.677517     0.0\n",
      "V9        0.580064       0.686844    0.631200     0.0\n",
      "V10       0.618778       0.755967    0.683941     0.0\n",
      "V11       0.506517       0.579026    0.541560     0.0\n",
      "V12       1.060766       1.213565    1.134596     0.0\n",
      "V13       0.595111       0.681372    0.636782     0.0\n",
      "V14       0.467669       0.529359    0.497559     0.0\n",
      "V15       0.325146       0.382259    0.352548     0.0\n",
      "V16       0.726179       0.854508    0.787735     0.0\n",
      "V17       0.463798       0.527309    0.494535     0.0\n",
      "V20       0.461445       0.560874    0.508736     0.0\n",
      "V21       1.629043       1.912838    1.765246     0.0\n",
      "V22       3.435136       4.379916    3.878867     0.0\n",
      "V23       1.205547       1.491012    1.340704     0.0\n",
      "V25       0.117744       0.156375    0.135692     0.0\n",
      "V26       1.181497       1.565834    1.360157     0.0\n",
      "V27       0.398484       0.562859    0.473593     0.0\n"
     ]
    }
   ],
   "source": [
    "params = np.exp(result.params)\n",
    "conf = np.exp(result.conf_int())\n",
    "conf['OR'] = params\n",
    "pvalue=round(result.pvalues,3)\n",
    "conf['pvalue']=pvalue\n",
    "conf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\n",
    "print ((conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features=card[['Time','V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V20','V21', 'V22', 'V23', 'V25', 'V26', 'V27','Class']]\n",
    "x=new_features.iloc[:,:-1]\n",
    "y=new_features.iloc[:,-1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred=logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85295\n",
      "           1       0.79      0.68      0.73       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.89      0.84      0.86     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=cm[0,0]\n",
    "TP=cm[1,1]\n",
    "FN=cm[1,0]\n",
    "FP=cm[0,1]\n",
    "sensitivity=TP/float(TP+FN)\n",
    "specificity=TN/float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =        0.9991222218320986 \n",
      " The Missclassification = 1-Accuracy =                   0.0008777781679013552 \n",
      " Sensitivity or True Positive Rate = TP/(TP+FN) =        0.6756756756756757 \n",
      " Specificity or True Negative Rate = TN/(TN+FP) =        0.9996834515505012 \n",
      " Positive Predictive value = TP/(TP+FP) =                0.7874015748031497 \n",
      " Negative predictive Value = TN/(TN+FN) =                0.9994373857189741 \n",
      " Positive Likelihood Ratio = Sensitivity/(1-Specificity) =  2134.509509509504 \n",
      " Negative likelihood Ratio = (1-Sensitivity)/Specificity =  0.3244270211948591\n"
     ]
    }
   ],
   "source": [
    "print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =       ',(TP+TN)/float(TP+TN+FP+FN),'\\n',\n",
    "\n",
    "'The Missclassification = 1-Accuracy =                  ',1-((TP+TN)/float(TP+TN+FP+FN)),'\\n',\n",
    "\n",
    "'Sensitivity or True Positive Rate = TP/(TP+FN) =       ',TP/float(TP+FN),'\\n',\n",
    "\n",
    "'Specificity or True Negative Rate = TN/(TN+FP) =       ',TN/float(TN+FP),'\\n',\n",
    "\n",
    "'Positive Predictive value = TP/(TP+FP) =               ',TP/float(TP+FP),'\\n',\n",
    "\n",
    "'Negative predictive Value = TN/(TN+FN) =               ',TN/float(TN+FN),'\\n',\n",
    "\n",
    "'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',sensitivity/(1-specificity),'\\n',\n",
    "\n",
    "'Negative likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-sensitivity)/specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob of Not Fraud (0)</th>\n",
       "      <th>Prob of Fraud (1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.999640</td>\n",
       "      <td>3.597437e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>9.383426e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>6.700988e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.726034e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.999476</td>\n",
       "      <td>5.242659e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prob of Not Fraud (0)  Prob of Fraud (1)\n",
       "0               0.999640       3.597437e-04\n",
       "1               0.999062       9.383426e-04\n",
       "2               0.999330       6.700988e-04\n",
       "3               1.000000       5.726034e-08\n",
       "4               0.999476       5.242659e-04"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob=logreg.predict_proba(x_test)[:,:]\n",
    "y_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of Not Fraud (0)','Prob of Fraud (1)'])\n",
    "y_pred_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 0.0 threshold the Confusion Matrix is  \n",
      " [[    0 85295]\n",
      " [    0   148]] \n",
      " with 148 correct predictions and 0 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  1.0 Specificity:  0.0 \n",
      "\n",
      "\n",
      "\n",
      "With 0.1 threshold the Confusion Matrix is  \n",
      " [[84732   563]\n",
      " [   43   105]] \n",
      " with 84837 correct predictions and 43 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7094594594594594 Specificity:  0.9933993786271177 \n",
      "\n",
      "\n",
      "\n",
      "With 0.2 threshold the Confusion Matrix is  \n",
      " [[85131   164]\n",
      " [   44   104]] \n",
      " with 85235 correct predictions and 44 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7027027027027027 Specificity:  0.998077261269711 \n",
      "\n",
      "\n",
      "\n",
      "With 0.3 threshold the Confusion Matrix is  \n",
      " [[85233    62]\n",
      " [   47   101]] \n",
      " with 85334 correct predictions and 47 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.6824324324324325 Specificity:  0.9992731109678176 \n",
      "\n",
      "\n",
      "\n",
      "With 0.4 threshold the Confusion Matrix is  \n",
      " [[85258    37]\n",
      " [   48   100]] \n",
      " with 85358 correct predictions and 48 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.6756756756756757 Specificity:  0.9995662113840201 \n",
      "\n",
      "\n",
      "\n",
      "With 0.5 threshold the Confusion Matrix is  \n",
      " [[85268    27]\n",
      " [   48   100]] \n",
      " with 85368 correct predictions and 48 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.6756756756756757 Specificity:  0.9996834515505012 \n",
      "\n",
      "\n",
      "\n",
      "With 0.6 threshold the Confusion Matrix is  \n",
      " [[85272    23]\n",
      " [   49    99]] \n",
      " with 85371 correct predictions and 49 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.668918918918919 Specificity:  0.9997303476170936 \n",
      "\n",
      "\n",
      "\n",
      "With 0.7 threshold the Confusion Matrix is  \n",
      " [[85274    21]\n",
      " [   51    97]] \n",
      " with 85371 correct predictions and 51 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.6554054054054054 Specificity:  0.9997537956503898 \n",
      "\n",
      "\n",
      "\n",
      "With 0.8 threshold the Confusion Matrix is  \n",
      " [[85278    17]\n",
      " [   54    94]] \n",
      " with 85372 correct predictions and 54 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.6351351351351351 Specificity:  0.9998006917169823 \n",
      "\n",
      "\n",
      "\n",
      "With 0.9 threshold the Confusion Matrix is  \n",
      " [[85282    13]\n",
      " [   61    87]] \n",
      " with 85369 correct predictions and 61 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.5878378378378378 Specificity:  0.9998475877835746 \n",
      "\n",
      "\n",
      "\n",
      "With 1.0 threshold the Confusion Matrix is  \n",
      " [[85295     0]\n",
      " [  148     0]] \n",
      " with 85295 correct predictions and 148 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.0 Specificity:  1.0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "for i in range(0,11):\n",
    "    cm2=0\n",
    "    y_pred_prob_yes=logreg.predict_proba(x_test)\n",
    "    y_pred2=binarize(y_pred_prob_yes,i/10)[:,1]\n",
    "    cm2=confusion_matrix(y_test,y_pred2)\n",
    "    print ('With',i/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n",
    "            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n",
    "          'Sensitivity: ',cm2[1,1]/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9184357785301569"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_pred_prob_yes[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All attributes selected after the elimination process show Pvalues lower than 5% and thereby suggesting significant role in the fraud Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85295\n",
      "           1       0.94      0.78      0.85       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.97      0.89      0.93     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#building the Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "#random forest model creation \n",
    "rfc = RandomForestClassifier() \n",
    "\n",
    "rfc.fit(x_train,y_train)\n",
    "y_pred_rf=rfc.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred_rf)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=cm[0,0]\n",
    "TP=cm[1,1]\n",
    "FN=cm[1,0]\n",
    "FP=cm[0,1]\n",
    "sensitivity=TP/float(TP+FN)\n",
    "specificity=TN/float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =        0.9995318516437859 \n",
      " The Missclassification = 1-Accuracy =                   0.0004681483562141153 \n",
      " Sensitivity or True Positive Rate = TP/(TP+FN) =        0.777027027027027 \n",
      " Specificity or True Negative Rate = TN/(TN+FP) =        0.9999179318834632 \n",
      " Positive Predictive value = TP/(TP+FP) =                0.9426229508196722 \n",
      " Negative predictive Value = TN/(TN+FN) =                0.9996132253489762 \n",
      " Positive Likelihood Ratio = Sensitivity/(1-Specificity) =  9468.074324319081 \n",
      " Negative likelihood Ratio = (1-Sensitivity)/Specificity =  0.22299127344678893\n"
     ]
    }
   ],
   "source": [
    "print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =       ',(TP+TN)/float(TP+TN+FP+FN),'\\n',\n",
    "\n",
    "'The Missclassification = 1-Accuracy =                  ',1-((TP+TN)/float(TP+TN+FP+FN)),'\\n',\n",
    "\n",
    "'Sensitivity or True Positive Rate = TP/(TP+FN) =       ',TP/float(TP+FN),'\\n',\n",
    "\n",
    "'Specificity or True Negative Rate = TN/(TN+FP) =       ',TN/float(TN+FP),'\\n',\n",
    "\n",
    "'Positive Predictive value = TP/(TP+FP) =               ',TP/float(TP+FP),'\\n',\n",
    "\n",
    "'Negative predictive Value = TN/(TN+FN) =               ',TN/float(TN+FN),'\\n',\n",
    "\n",
    "'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',sensitivity/(1-specificity),'\\n',\n",
    "\n",
    "'Negative likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-sensitivity)/specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
